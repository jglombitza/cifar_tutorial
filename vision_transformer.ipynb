{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jglombitza/cifar_tutorial/blob/main/vision_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "454dab96",
      "metadata": {
        "id": "454dab96"
      },
      "source": [
        "# Visition transformer example for the CIFAR-10 classification task.\n",
        "Classify image samples of the CIFAR-10 dataset using a transformer network\n",
        "\n",
        "Learn more about vision transformers (ViTs) at https://arxiv.org/abs/2010.11929\n",
        "Learn more about the attention mechanism, the basic building block of each transformer, at https://arxiv.org/abs/1706.03762\n",
        "\n",
        "## CIFAR-10 Dataset\n",
        "The CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset.\n",
        "They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\n",
        "see http://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "Contains 6 files, each with 10000 shuffled images of 10 labeled classes.\n",
        "\n",
        "The images are of size 32x32 pixels with 3 color channels (RGB).\n",
        "The intensity in each channels is encoded as unsigned 8-bit integers 0...255.\n",
        "\n",
        "\n",
        "Code partly adapted from https://keras.io/examples/vision/image_classification_with_vision_transformer/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0e41b5a9",
      "metadata": {
        "id": "0e41b5a9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "\n",
        "layers = tf.keras.layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4200a5f4",
      "metadata": {
        "id": "4200a5f4"
      },
      "source": [
        "## Load and preprocess CIFAR-10 data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "41a1c492",
      "metadata": {
        "id": "41a1c492",
        "outputId": "7ac221ae-6472-4818-9b1f-7233aea97a9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images, shape =  (50000, 32, 32, 3)\n",
            "labels, shape =  (50000, 1)\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "print(\"images, shape = \", x_train.shape)\n",
        "print(\"labels, shape = \", y_train.shape)\n",
        "input_shape = (32, 32, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "\n",
        "def plot_patches(image, image_size, patch_size, outfile):\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(image.astype(\"uint8\"))\n",
        "    plt.axis(\"off\")\n",
        "    plt.savefig(outfile)\n",
        "\n",
        "    patches = Patches(patch_size)(image[tf.newaxis, ...])\n",
        "    print(patches.shape)\n",
        "    print(f\"Image size: {image_size} X {image_size}\")\n",
        "    print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "    print(f\"Patches per image: {patches.shape[1]}\")\n",
        "    print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "    n = int(np.sqrt(patches.shape[1]))\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    for i, patch in enumerate(patches[0]):\n",
        "        ax = plt.subplot(n, n, i + 1)\n",
        "        patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
        "        plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xeRNEFCQr9il"
      },
      "id": "xeRNEFCQr9il",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the data (that is divided into patches)"
      ],
      "metadata": {
        "id": "q3HKblQbsN7_"
      },
      "id": "q3HKblQbsN7_"
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = input_shape[0]\n",
        "patch_size = 4  # Size of the patches\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
        "plot_patches(image, image_size, patch_size, \"image.png\")\n"
      ],
      "metadata": {
        "id": "mM9hhGvtsC1W",
        "outputId": "1bda7f31-0717-4b3b-97cb-0d367c827f2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        }
      },
      "id": "mM9hhGvtsC1W",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 64, 48)\n",
            "Image size: 32 X 32\n",
            "Patch size: 4 X 4\n",
            "Patches per image: 64\n",
            "Elements per patch: 48\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 64 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aa7cdfc",
      "metadata": {
        "id": "4aa7cdfc"
      },
      "source": [
        "### Pre-process data\n",
        "normalize each pixel and color-channel separately across all images\n",
        "take 2000 images for validation from test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9ed0685a",
      "metadata": {
        "id": "9ed0685a"
      },
      "outputs": [],
      "source": [
        "def normalize(images):\n",
        "    mean = np.mean(images, axis=0)[np.newaxis]  # shape = (1, 32, 32, 3)\n",
        "    sigma = np.std(images, axis=0)[np.newaxis]  # shape = (1, 32, 32, 3)\n",
        "    images_normalized = (images - mean) / sigma\n",
        "    return images_normalized\n",
        "\n",
        "# take 2000 images for validation from test data\n",
        "x_train_norm = normalize(x_train)\n",
        "x_test_norm = normalize(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_onehot = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test_onehot = tf.keras.utils.to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "UncPyJ0EuqCT"
      },
      "id": "UncPyJ0EuqCT",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8d47a4fb",
      "metadata": {
        "id": "8d47a4fb"
      },
      "source": [
        "## Define model\n",
        "Add new transformer neural network here!\n",
        "\n",
        "First start with definining the PatchEncoder --> Positional encoding."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n"
      ],
      "metadata": {
        "id": "t7cewj8oqTe8"
      },
      "id": "t7cewj8oqTe8",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we define the feed-forward MLP that is used in each transformer block"
      ],
      "metadata": {
        "id": "YoeviUD6qjL7"
      },
      "id": "YoeviUD6qjL7"
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(x, hidden_units, dropout_rate, transformer_units):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    x = layers.Dense(transformer_units, activation=tf.nn.gelu)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "aK8WL7ACqd_P"
      },
      "id": "aK8WL7ACqd_P",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we start setting up the transformer..."
      ],
      "metadata": {
        "id": "MoWaymXlqhjc"
      },
      "id": "MoWaymXlqhjc"
    },
    {
      "cell_type": "code",
      "source": [
        "# Small values to enable fast training -> will not achieve good performance\n",
        "\n",
        "projection_dim = 32  # Size of transformer layers (d)\n",
        "transformer_layers = 4  # number of transformer blocks\n",
        "num_heads = 4  # remember that d has to be dividable by num_heads (h)\n",
        "mlp_head_units = [128]  # Size of MLP hidden layer"
      ],
      "metadata": {
        "id": "d9yBlpsrslFJ"
      },
      "id": "d9yBlpsrslFJ",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b773442",
      "metadata": {
        "id": "6b773442"
      },
      "outputs": [],
      "source": [
        "# Build model\n",
        "inputs = layers.Input(shape=input_shape)\n",
        "patches = Patches(patch_size)(inputs)\n",
        "# Encode patches.\n",
        "encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "# Construct multiple layers of the Transformer block.\n",
        "for _ in range(transformer_layers):\n",
        "\n",
        "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)  # Layernorm 1\n",
        "    # TASK: perform attention\n",
        "    # make use of \"num_heads\" and \"projection_dim\"\n",
        "    # perfrom self attention using the MultiHeadAttention layer.\n",
        "    attention_output = ....\n",
        "\n",
        "    x2 = layers.Add()([attention_output, encoded_patches])  # Residual connection\n",
        "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)  # Layernorm 2\n",
        "\n",
        "    # TASK: add MLP model\n",
        "    x3 = ....\n",
        "    encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "\n",
        "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "representation = layers.Flatten()(representation)\n",
        "representation = layers.Dropout(0.3)(representation)\n",
        "\n",
        "# Create output of the model\n",
        "# Add a Dense, Dropout, and a final classification layer (remember num_classes = 10 in CIFAR-10)\n",
        "output = ...\n",
        "\n",
        "# Build the Keras model.\n",
        "model = keras.Model(inputs=inputs, outputs=output, name=\"vit\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare model for training"
      ],
      "metadata": {
        "id": "_O1tEOPV6LlT"
      },
      "id": "_O1tEOPV6LlT"
    },
    {
      "cell_type": "code",
      "source": [
        "# setup the optimizer\n",
        "learning_rate = 0.001\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)"
      ],
      "metadata": {
        "id": "Shmp8Shmsz-y"
      },
      "id": "Shmp8Shmsz-y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdf5e8d8",
      "metadata": {
        "id": "fdf5e8d8"
      },
      "outputs": [],
      "source": [
        "# compile the model\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=keras.losses.CategoricalCrossentropy(),\n",
        "    metrics=[keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "             keras.metrics.TopKCategoricalAccuracy(3, name=\"top-3-accuracy\")])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform training\n",
        "\n",
        "Transformer need lots of data.\n",
        "Thus, expect the CNN to perform better than transformers that are not pre-trained."
      ],
      "metadata": {
        "id": "BJP-PCjt6FAf"
      },
      "id": "BJP-PCjt6FAf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f43014c",
      "metadata": {
        "id": "3f43014c"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "num_epochs = 20\n",
        "\n",
        "history = model.fit(\n",
        "    x=x_train_norm,\n",
        "    y=y_train_onehot,\n",
        "    batch_size=batch_size,\n",
        "    epochs=num_epochs,\n",
        "    validation_split=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "487042ed",
      "metadata": {
        "id": "487042ed"
      },
      "source": [
        "## Plot training curves"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2be85637",
      "metadata": {
        "id": "2be85637"
      },
      "source": [
        "### Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d41da33",
      "metadata": {
        "id": "5d41da33"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1)\n",
        "nb_epochs = np.arange(len(history.history[\"loss\"]))\n",
        "ax.plot(nb_epochs, history.history[\"loss\"], label=\"train\")\n",
        "ax.plot(nb_epochs, history.history[\"val_loss\"], label=\"validation\")\n",
        "ax.legend()\n",
        "ax.set(xlabel=\"epoch\", ylabel=\"loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5d4cb4d",
      "metadata": {
        "id": "f5d4cb4d"
      },
      "source": [
        "### Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a443b137",
      "metadata": {
        "id": "a443b137"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1)\n",
        "nb_epochs = np.arange(len(history.history[\"accuracy\"]))\n",
        "ax.plot(nb_epochs, history.history[\"accuracy\"], label=\"training\")\n",
        "ax.plot(nb_epochs, history.history[\"val_accuracy\"], label=\"validation\")\n",
        "ax.legend()\n",
        "ax.set(xlabel=\"epoch\", ylabel=\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8214047",
      "metadata": {
        "id": "e8214047"
      },
      "source": [
        "## Plot classifications using the test set\n",
        "calculate predictions for test set and convert back to class labels (0-9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d083a911",
      "metadata": {
        "id": "d083a911"
      },
      "outputs": [],
      "source": [
        "def plot_prediction(X, Y, Y_predict, fname=False):\n",
        "    labels = np.array([\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"])\n",
        "\n",
        "    \"\"\"\n",
        "    Plot image X along with predicted probabilities Y_predict.\n",
        "    X: CIFAR image, shape = (32, 32, 3)\n",
        "    Y: CIFAR label, one-hot encoded, shape = (10)\n",
        "    Y_predict: predicted probabilities, shape = (10)\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
        "\n",
        "    # plot image\n",
        "    ax1.imshow(X.astype(\"uint8\"), origin=\"upper\")\n",
        "    ax1.set(xticks=[], yticks=[])\n",
        "\n",
        "    # plot probabilities\n",
        "    ax2.barh(np.arange(10), Y_predict, align=\"center\")\n",
        "    ax2.set(xlim=(0, 1), xlabel=\"Score\", yticks=[])\n",
        "    for i in range(10):\n",
        "        c = \"red\" if (i == np.argmax(Y)) else \"black\"\n",
        "        ax2.text(0.05, i, labels[i].capitalize(), ha=\"left\", va=\"center\", color=c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22b25faa",
      "metadata": {
        "id": "22b25faa"
      },
      "outputs": [],
      "source": [
        "y_predict = model.predict(x_test_norm, batch_size=128)\n",
        "\n",
        "y_predict_cl = np.argmax(y_predict, axis=1)\n",
        "y_test_cl = np.argmax(y_test_onehot, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8438a56",
      "metadata": {
        "id": "a8438a56"
      },
      "outputs": [],
      "source": [
        "m = y_predict_cl == y_test_cl\n",
        "i0 = np.arange(10000)[~m]  # misclassified images\n",
        "i1 = np.arange(10000)[m]  # correctly classified images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "791a57f7",
      "metadata": {
        "id": "791a57f7"
      },
      "source": [
        "#### Plot first 5 false classifications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4989cb3d",
      "metadata": {
        "scrolled": false,
        "id": "4989cb3d"
      },
      "outputs": [],
      "source": [
        "fname = \"false_%i_{}.png\".format(model.name)\n",
        "for i in i0[0:5]:\n",
        "    plot_prediction(x_test[i], y_test_onehot[i], y_predict[i], fname=fname % i)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b5f21e9",
      "metadata": {
        "id": "7b5f21e9"
      },
      "source": [
        "#### Plot first 5 correct classifications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d053bce5",
      "metadata": {
        "scrolled": false,
        "id": "d053bce5"
      },
      "outputs": [],
      "source": [
        "fname = \"correct_%i_{}.png\".format(model.name)\n",
        "for i in i1[0:5]:\n",
        "    plot_prediction(x_test[i], y_test_onehot[i], y_predict[i], fname=fname % i)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vgguJ9Jh2b0d"
      },
      "id": "vgguJ9Jh2b0d",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}